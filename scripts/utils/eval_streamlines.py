#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys

# # Hack so you don't have to put the library containing this script in the PYTHONPATH.
# sys.path = [os.path.abspath(os.path.join(__file__, '..', '..'))] + sys.path

import numpy as np
from os.path import join as pjoin
import argparse
import itertools
import theano
import time
import nibabel as nib
from nibabel.streamlines import ArraySequence, Tractogram

from smartlearner import views
from smartlearner.status import Status
from smartlearner import utils as smartutils

from learn2track.utils import Timer, log_variables
from learn2track.factories import model_factory
from learn2track.factories import loss_factory

from learn2track import datasets
from learn2track import utils
from learn2track import batch_schedulers
from learn2track.neurotools import VolumeManager


def build_parser():
    DESCRIPTION = ("Save streamlines along their loss value.")
    p = argparse.ArgumentParser(description=DESCRIPTION)

    # General options (optional)
    p.add_argument('name', help='name/path of the experiment.')
    p.add_argument('subject', help='file containing validation data (as generated by `process_streamlines.py`).')
    p.add_argument('--batch_size', type=int, default=200,
                   help='size of the batch.')

    p.add_argument('--out', default="tractogram.trk", help='output filename. Default: %(default)s')

    p.add_argument('-f', '--force', action='store_true', help='restart training from scratch instead of resuming.')
    return p


def main():
    parser = build_parser()
    args = parser.parse_args()
    print(args)

    # Get experiment folder
    experiment_path = args.name
    if not os.path.isdir(experiment_path):
        # If not a directory, it must be the name of the experiment.
        experiment_path = pjoin(".", "experiments", args.name)

    if not os.path.isdir(experiment_path):
        parser.error('Cannot find experiment: {0}!'.format(args.name))

    # Load experiments hyperparameters
    try:
        hyperparams = smartutils.load_dict_from_json_file(pjoin(experiment_path, "hyperparams.json"))
    except FileNotFoundError:
        hyperparams = smartutils.load_dict_from_json_file(pjoin(experiment_path, "..", "hyperparams.json"))

    with Timer("Loading dataset", newline=True):
        volume_manager = VolumeManager()
        dataset = datasets.load_tractography_dataset([args.subject], volume_manager, name="dataset", use_sh_coeffs=hyperparams['use_sh_coeffs'])
        print("Dataset size:", len(dataset))

    with Timer("Loading model"):
        if hyperparams['model'] == 'gru_regression':
            from learn2track.models import GRU_Regression
            model = GRU_Regression.create(experiment_path, volume_manager=volume_manager)
        else:
            raise NameError("Unknown model: {}".format(hyperparams['model']))

    with Timer("Building evaluation function"):
        loss = loss_factory(hyperparams, model, dataset)
        batch_scheduler = batch_schedulers.TractographyBatchScheduler(dataset,
                                                                      batch_size=1000,
                                                                      noisy_streamlines_sigma=None,
                                                                      use_data_augment=False,  # Otherwise it doubles the number of losses :-/
                                                                      seed=1234,
                                                                      shuffle_streamlines=False,
                                                                      normalize_target=hyperparams['normalize'])

        loss_view = views.LossView(loss=loss, batch_scheduler=batch_scheduler)
        losses = loss_view.losses.view()

    with Timer("Saving streamlines"):
        tractogram = Tractogram(dataset.streamlines, affine_to_rasmm=dataset.subjects[0].signal.affine)
        tractogram.data_per_streamline['loss'] = losses
        nib.streamlines.save(tractogram, args.out)


if __name__ == "__main__":
    main()
