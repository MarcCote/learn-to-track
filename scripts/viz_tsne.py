#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys

# Hack so you don't have to put the library containing this script in the PYTHONPATH.
sys.path = [os.path.abspath(os.path.join(__file__, '..', '..'))] + sys.path

import numpy as np
import argparse
from os.path import join as pjoin

import theano
import theano.tensor as T

from learn2track import utils
from learn2track.utils import Timer

from learn2track import datasets
from learn2track.batch_schedulers import TractographyBatchScheduler
from learn2track.neurotools import VolumeManager


# def build_train_gru_argparser(subparser):
#     DESCRIPTION = "Train a GRU."

#     p = subparser.add_parser("gru_regression", description=DESCRIPTION, help=DESCRIPTION)

#     # p.add_argument('dataset', type=str, help='folder containing training data (.npz files).')

#     # Model options (GRU)
#     model = p.add_argument_group("GRU arguments")

#     model.add_argument('--hidden-sizes', type=int, nargs='+', default=500,
#                        help="Size of the hidden layers. Default: 500")

#     model.add_argument('--weights-initialization', type=str, default='orthogonal', choices=WEIGHTS_INITIALIZERS,
#                        help='which type of initialization to use when creating weights [{0}].'.format(", ".join(WEIGHTS_INITIALIZERS)))
#     model.add_argument('--initialization-seed', type=int, default=1234,
#                        help='seed used to generate random numbers. Default=1234')

#     model.add_argument('--learn-to-stop', action="store_true",
#                        help='if specified, the model will be trained to learn when to stop tracking')

#     model.add_argument('--normalize', action="store_true",
#                        help='if specified, targets and the output direction the model produces will have unit length.')

#     model.add_argument('--feed-previous-direction', action="store_true",
#                        help='if specified, the model will be given the previous direction as an additional input')

#     model.add_argument('--predict-offset', action="store_true",
#                        help=('if specified, the model will predict the offset from the previous direction instead',
#                              ' (need --feed-previous-direction)'))

#     # General parameters (optional)
#     general = p.add_argument_group("General arguments")
#     general.add_argument('-f', '--force', action='store_true', help='restart training from scratch instead of resuming.')
#     general.add_argument('--view', action='store_true', help='display learning curves.')


def build_argparser():
    DESCRIPTION = ("Script performing T-SNE visualization of some datasets.")
    p = argparse.ArgumentParser(description=DESCRIPTION)

    p.add_argument('--nb-streamlines-per-subject', type=int, default=2000,
                   help='nb of streamlines per subject to use. Default: %(default)s.')

    # Dataset options
    dataset = p.add_argument_group("Data options")
    dataset.add_argument('subjects', nargs='+',
                         help='file containing .npz data (as generated by `process_streamlines.py`).')
    dataset.add_argument('--use-sh-coeffs', action='store_true',
                         help='if specified, use Spherical Harmonic coefficients as inputs to the model. Default: dwi coefficients.')

    training = p.add_argument_group("Training options")
    training.add_argument('--noisy-streamlines-sigma', type=float,
                          help='if specified, it is the standard deviation of the gaussian noise added independently to every point of every streamlines at each batch.')
    training.add_argument('--keep-step-size', action="store_true",
                          help='if specified, training streamlines will not be resampled between batches (streamlines will keep their original step size)')
    training.add_argument('--sort-streamlines', action="store_true",
                          help='if specified, streamlines will be approximatively regrouped according to their lengths. (Training speedup).')

    return p


def main():
    parser = build_argparser()
    args = parser.parse_args()
    print(args)
    print("Using Theano v.{}".format(theano.version.short_version))

    hyperparams_to_exclude = ['max_epoch', 'force', 'name', 'view', 'shuffle_streamlines']
    # Use this for hyperparams added in a new version, but nonexistent from older versions
    retrocompatibility_defaults = {'feed_previous_direction': False,
                                   'predict_offset': False,
                                   'normalize': False,
                                   'sort_streamlines': False,
                                   'keep_step_size': False}
    experiment_path, hyperparams, resuming = utils.maybe_create_experiment_folder(args, exclude=hyperparams_to_exclude,
                                                                                  retrocompatibility_defaults=retrocompatibility_defaults)

    # Log the command currently running.
    with open(pjoin(experiment_path, 'cmd.txt'), 'a') as f:
        f.write(" ".join(sys.argv) + "\n")

    print("Resuming:" if resuming else "Creating:", experiment_path)

    with Timer("Loading dataset", newline=True):
        volume_manager = VolumeManager()
        dataset = datasets.load_tractography_dataset(args.subjects, volume_manager, name="dataset", use_sh_coeffs=args.use_sh_coeffs)
        print("Total streamlines: {}".format(len(dataset)))

    with Timer("Running T-SNE", newline=True):

        batch_scheduler = TractographyBatchScheduler(dataset,
                                                     batch_size=min(len(dataset), 100000),
                                                     noisy_streamlines_sigma=False,
                                                     seed=1234,
                                                     normalize_target=True)
        rng = np.random.RandomState(42)
        rng.shuffle(batch_scheduler.indices)

        # bundle_name_pattern = "CST_Left"
        # batch_inputs, batch_targets, batch_mask = batch_scheduler._prepare_batch(trainset.get_bundle(bundle_name_pattern, return_idx=True))
        inputs, _, mask = batch_scheduler._next_batch(0)

        # Keep the same number of streamlines per subject
        new_inputs = []
        new_mask = []
        for i in range(len(args.subject)):
            subset = inputs[:, 0, -1] == i
            if len(subset.sum()) < args.nb_streamlines_per_subject:
                raise NameError("Not enough streamlines for subject #{}".format(i))

            new_inputs += [inputs[subset][:args.nb_streamlines_per_subject]]
            new_mask += [mask[subset][:args.nb_streamlines_per_subject]]

        inputs = np.concatenate([new_inputs], axis=0)
        mask = np.concatenate([new_mask], axis=0)

        mask = mask.astype(bool)
        idx = np.arange(mask.sum())
        rng.shuffle(idx)

        coords = T.matrix('coords')
        eval_at_coords = theano.function([coords], volume_manager.eval_at_coords(coords))

        coords = inputs[mask][idx]
        X = eval_at_coords(coords)

        from sklearn.manifold.t_sne import TSNE
        tsne = TSNE(n_components=2, verbose=2, random_state=42)
        Y = tsne.fit_transform(X)

        import matplotlib.pyplot as plt
        plt.figure()
        ids = range(len(dataset.subjects))
        markers = ['s', 'o', '^', 'v', '<', '>', 'h']
        colors = ['cyan', 'darkorange', 'darkgreen', 'magenta', 'pink', 'k']
        for i, marker, color in zip(ids, markers, colors):
            idx = coords[:, -1] == i
            print("Subject #{}: ".format(i), idx.sum())
            plt.scatter(Y[idx, 0], Y[idx, 1], 20, color=color, marker=marker, label="Subject #{}".format(i))

        plt.legend()
        plt.show()


if __name__ == "__main__":
    main()
